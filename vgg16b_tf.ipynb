{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg16b_tf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akoo-45/nevis-CNN/blob/master/vgg16b_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKzDcZjjij-s",
        "colab_type": "text"
      },
      "source": [
        "This notebook runs a VGG16b convolutional neural networks for classifying 5 particle images in a simulated LArTPC detector available from the [public dataset](http://deeplearnphysics.org/DataChallenge/). We use Tensorflow to train the network and larcv_threadio to fetch data from larcv files. \n",
        "\n",
        "To run the file: \n",
        "\n",
        "\n",
        "```\n",
        "ssh hopper\n",
        "cd /data/ashley.koo/larcv-tutorial\n",
        "# Download necessary libraries (see appendix in [writeup]\n",
        "(https://docs.google.com/document/d/1jElkhcZG15OG6Azza3dgda2aagX1vHUS5YlMw8LcOBc/edit)) \n",
        "python akoo_vgg16b_tf.py\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9AVLpLyeiM4",
        "colab_type": "text"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJJiJXnggG2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from larcv import larcv\n",
        "from larcv.dataloader2 import larcv_threadio\n",
        "import numpy as np\n",
        "import os,sys,time\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim # useful library for defining complex models like VGG16b with repeated layers\n",
        "import tensorflow.python.platform\n",
        "\n",
        "\n",
        "# tensorflow/gpu start-up configuration\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNdNjVuChLG5",
        "colab_type": "text"
      },
      "source": [
        "[Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) is useful for VGG16 architectures since there are repeated layers with same parameters (Slim has a handy .repeat() method for doing that)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xnkvuAfhdr",
        "colab_type": "text"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjO5EVKYezyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make directory paths for the larcv IOs\n",
        "TUTORIAL_DIR     = '.'\n",
        "TRAIN_IO_CONFIG  = os.path.join(TUTORIAL_DIR, 'tf/io_train.cfg') # configuration file stored in './tf/io_train.cfg'\n",
        "TEST_IO_CONFIG   = os.path.join(TUTORIAL_DIR, 'tf/io_test.cfg' ) # configuration file stored in './tf/io_test.cfg'\n",
        "TRAIN_BATCH_SIZE = 10\n",
        "TEST_BATCH_SIZE  = 100\n",
        "LOGDIR           = 'log'\n",
        "ITERATIONS       = 5000\n",
        "SAVE_SUMMARY     = 20\n",
        "SAVE_WEIGHTS     = 100\n",
        "\n",
        "# Check that the log directory is empty  \n",
        "train_logdir = os.path.join(LOGDIR,'train')\n",
        "test_logdir  = os.path.join(LOGDIR,'test')\n",
        "# Make new log directory\n",
        "if not os.path.isdir(train_logdir): os.makedirs(train_logdir)\n",
        "if not os.path.isdir(test_logdir):  os.makedirs(test_logdir)\n",
        "# Raise error \n",
        "if len(os.listdir(train_logdir)) or len(os.listdir(test_logdir)):\n",
        "  sys.stderr.write('Error: train or test log dir not empty...\\n')\n",
        "  raise OSError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5z_e6-ZgSfG",
        "colab_type": "text"
      },
      "source": [
        "# Configure data reader\n",
        "We prepare two data reader instances: one for training and another for testing the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUAqwL61gbnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Step 0: IO\n",
        "#\n",
        "# train dataset\n",
        "train_io = larcv_threadio()  # create io interface\n",
        "train_io_cfg = {'filler_name' : 'TrainIO',\n",
        "                'verbosity'   : 10,\n",
        "                'filler_cfg'  : TRAIN_IO_CONFIG}\n",
        "train_io.configure(train_io_cfg)   # configure\n",
        "train_io.start_manager(TRAIN_BATCH_SIZE) # start read thread\n",
        "time.sleep(2)\n",
        "train_io.next()\n",
        "\n",
        "# test dataset\n",
        "test_io = larcv_threadio()   # create io interface\n",
        "test_io_cfg = {'filler_name' : 'TestIO',\n",
        "               'verbosity'   : 10,\n",
        "               'filler_cfg'  : TEST_IO_CONFIG}\n",
        "test_io.configure(test_io_cfg)   # configure\n",
        "test_io.start_manager(TEST_BATCH_SIZE) # start read thread\n",
        "time.sleep(2)\n",
        "test_io.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f04kNpU5iZH8",
        "colab_type": "text"
      },
      "source": [
        "#Defining a network\n",
        "We use 16 convolution layers with max-pooling operation followed after every 2 convolution layers except the last layer is average-pooling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAoWURabnBn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "# Step 1: Define Network\n",
        "#\n",
        "def build(inputs, num_class=4, trainable=True, debug=True):\n",
        "  filters = 64\n",
        "  with slim.arg_scope([slim.conv2d, slim.fully_connected], trainable=trainable,\n",
        "                      activation_fn=tf.nn.relu):\n",
        "    # conv1\n",
        "    net = slim.conv2d(inputs, filters, [3, 3], stride=2, scope='conv1_1')\n",
        "    net = slim.conv2d(net, filters, [3, 3], stride=1, scope='conv1_2')\n",
        "\t   # pool1 - stride2\n",
        "    net = slim.max_pool2d(net, [2, 2], stride = 2, scope='pool1')\n",
        "    filters *= 2 # num features X 2\n",
        "\n",
        "    with slim.arg_scope([slim.conv2d], trainable=trainable, stride=1):\n",
        "      # conv2\n",
        "      net = slim.repeat(net, 2, slim.conv2d, filters, [3, 3], scope='conv2') # instead of conv2_1, conv2_2\n",
        "\t     # pool2 - stride2\n",
        "      net = slim.max_pool2d(net, [2, 2], stride = 2, scope='pool2')\n",
        "      filters *= 2\n",
        "\n",
        "      # conv3\n",
        "      net = slim.repeat(net, 3, slim.conv2d, filters, [3, 3], scope='conv3') # instead of conv2_1, conv2_2\n",
        "\t     # pool3 - stride2\n",
        "      net = slim.max_pool2d(net, [2, 2], stride = 2, scope='pool3')\n",
        "      filters *= 2\n",
        "\n",
        "      # conv4\n",
        "      net = slim.repeat(net, 3, slim.conv2d, filters, [3, 3], scope='conv4') # instead of conv2_1, conv2_2\n",
        "        # pool4 - stride2\n",
        "      net = slim.max_pool2d(net, [2, 2], stride = 2, scope='pool4')\n",
        "      print(\"After step 4\", net.shape) \n",
        "\n",
        "      # conv5\n",
        "      net = slim.repeat(net, 3, slim.conv2d, filters, [3, 3], scope='conv5') # instead of conv2_1, conv2_2\n",
        "\t     # pool5 - stride2\n",
        "      net = slim.max_pool2d(net, [2, 2], stride = 2, scope='pool5')\n",
        "      print(\"After step 5\", net.shape) \n",
        "\n",
        "    #net = slim.dropout(net, 0.5, scope='dropout6')\n",
        "    #net = slim.fully_connected(net, 4096, scope='fc7')\n",
        "    \n",
        "    with tf.variable_scope('final'):\n",
        "      net = slim.flatten(net, scope='flatten')\n",
        "\n",
        "      if debug: print('After flattening', net.shape)\n",
        "      print('After flattening', net.shape) \n",
        "\n",
        "      net = slim.fully_connected(net, int(num_class), scope='fc6')\n",
        "\n",
        "      if debug: print('After final_fc', net.shape)\n",
        "      print('After final_fc', net.shape)\n",
        "\n",
        "      # NO SOFT MAX CALC.\n",
        "      return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFzbH1sdCBXI",
        "colab_type": "text"
      },
      "source": [
        "# Build the Network\n",
        "\n",
        "Build the network and define loss, accuracy metrics and our solver. Any optimizer should work but you may have to tune the parameters by yourself. Here, we use RMSPropOptimizer with base learning rate 0.0005 with no justification. Note we add minimal set of tensorflow variables into tf.summary to demonstrate later the tensorboard, a dedicated monitoring/visualization tool for network training with tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLNjcnVam-Bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Step 2: Build network + define loss & solver\n",
        "#\n",
        "# retrieve dimensions of data for network construction\n",
        "dim_data  = train_io.fetch_data('train_image').dim() \n",
        "dim_label = train_io.fetch_data('train_label').dim() \n",
        "\n",
        "# define place holders\n",
        "data_tensor    = tf.placeholder(tf.float32, [None, dim_data[1] * dim_data[2] * dim_data[3]], name='image')\n",
        "label_tensor   = tf.placeholder(tf.float32, [None, dim_label[1]], name='label')\n",
        "data_tensor_2d = tf.reshape(data_tensor, [-1,dim_data[1],dim_data[2],dim_data[3]],name='image_reshape')\n",
        "\n",
        "# Let's keep 10 random set of images in the log\n",
        "tf.summary.image('input',data_tensor_2d,10)\n",
        "# build net\n",
        "net = build(inputs=data_tensor_2d, trainable=True, num_class=dim_label[1], debug=False)\n",
        "\n",
        "# Define accuracy\n",
        "with tf.name_scope('accuracy'):\n",
        "  correct_prediction = tf.equal(tf.argmax(net,1), tf.argmax(label_tensor,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  tf.summary.scalar('accuracy', accuracy)\n",
        "  # Define loss + backprop as training step\n",
        "  with tf.name_scope('train'):\n",
        "    print('label_tensor', label_tensor)\n",
        "    print('logits', net)\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label_tensor, logits=net))\n",
        "    tf.summary.scalar('cross_entropy',cross_entropy)\n",
        "    train_step = tf.train.RMSPropOptimizer(0.00005).minimize(cross_entropy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpbjx6moCQAT",
        "colab_type": "text"
      },
      "source": [
        "# Defining tensorflow IO\n",
        "\n",
        "In the next cell we define tensorflow's IO\n",
        "\n",
        "merged_summary is a tensorflow operation to create summaries to be written into a log file for tensorboard.\n",
        "writer_train  writes monitoring data for training data sample into a log file.\n",
        "writer_test is the same as writer_train except it is for testing data sample.\n",
        "saver is a handle to store the state of the network = trained network variable values (weights, biases, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ya_RLZcm5PS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#                                                                                                                                      \n",
        "# Step 3: weight saver & summary writer                                                                                                \n",
        "#                                                                                                                                      \n",
        "# Create a bandle of summary                                                                                                           \n",
        "merged_summary=tf.summary.merge_all()\n",
        "# Create a session                                                                                                                     \n",
        "sess = tf.InteractiveSession()\n",
        "# Initialize variables                                                                                                                 \n",
        "sess.run(tf.global_variables_initializer())\n",
        "# Create a summary writer handle                                                                                                       \n",
        "writer_train=tf.summary.FileWriter(train_logdir)\n",
        "writer_train.add_graph(sess.graph)\n",
        "writer_test=tf.summary.FileWriter(test_logdir)\n",
        "writer_test.add_graph(sess.graph)\n",
        "# Create weights saver                                                                                                                 \n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFOgnXaHCck4",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3igeyYIm3UA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Step 4: Run training loop\n",
        "#\n",
        "for i in range(ITERATIONS):\n",
        "\n",
        "    train_data  = train_io.fetch_data('train_image').data()\n",
        "    train_label = train_io.fetch_data('train_label').data()\n",
        "\n",
        "    feed_dict = { data_tensor  : train_data,\n",
        "                  label_tensor : train_label }\n",
        "\n",
        "    loss, acc, _ = sess.run([cross_entropy, accuracy, train_step], feed_dict=feed_dict)\n",
        "\n",
        "    if (i+1)%SAVE_SUMMARY == 0:\n",
        "      # Save train log\n",
        "      sys.stdout.write('Training in progress @ step %d loss %g accuracy %g          \\n' % (i,loss,acc))\n",
        "      sys.stdout.flush()\n",
        "      s = sess.run(merged_summary, feed_dict=feed_dict)\n",
        "      writer_train.add_summary(s,i)\n",
        "      \n",
        "      # Calculate & save test log\n",
        "      test_data  = test_io.fetch_data('test_image').data()\n",
        "      test_label = test_io.fetch_data('test_label').data()\n",
        "      feed_dict  = { data_tensor  : test_data,\n",
        "                       label_tensor : test_label }\n",
        "      loss, acc = sess.run([cross_entropy, accuracy], feed_dict=feed_dict)\n",
        "      sys.stdout.write('Testing in progress @ step %d loss %g accuracy %g          \\n' % (i,loss,acc))\n",
        "      sys.stdout.flush()\n",
        "      s = sess.run(merged_summary, feed_dict=feed_dict)\n",
        "      writer_test.add_summary(s,i)\n",
        "        \n",
        "    test_io.next()\n",
        "\n",
        "    train_io.next()\n",
        "\n",
        "    if (i+1)%SAVE_WEIGHTS == 0:\n",
        "      ssf_path = saver.save(sess,'weights/toynet',global_step=i)\n",
        "      print('saved @',ssf_path)\n",
        "\n",
        "# inform log directory\n",
        "print()\n",
        "print('Run `tensorboard --logdir=%s` in terminal to see the results.' % LOGDIR)\n",
        "train_io.reset()\n",
        "test_io.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}